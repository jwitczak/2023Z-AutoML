{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from itertools import product\n",
    "from Utils.dataManagingUtils import save_prediction_to_file, save_data_to_csv, print_results, print_best_results, searchMaxRowByColumn\n",
    "from Utils.transformersUtils import OryginalData, PCATransformer, CORRTransformer, LassoSelector, RFCSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wartości zmiennych \n",
    "n_jobs=-1\n",
    "n_iter_randomSearch_rf=1000\n",
    "n_iter_randomSearch_sv=1000\n",
    "n_iter_randomSearch_gb=1000\n",
    "n_iter_randomSearch_dt=10000\n",
    "n_iter_randomSearch_final = 5000\n",
    "verbose=1\n",
    "random_state=42\n",
    "cv=7\n",
    "numberOfPoints=25\n",
    "\n",
    "# Wartości hiperparametrów\n",
    "n_estimators_lower_rf = 5\n",
    "n_estimators_upper_rf = 100\n",
    "max_depth_lower_rf = 1\n",
    "max_depth_upper_rf = 10\n",
    "min_samples_split_lower_rf = 1\n",
    "min_samples_split_upper_rf = 5\n",
    "min_samples_leaf_lower_rf = 1\n",
    "min_samples_leaf_upper_rf = 10\n",
    "max_features_rf = ['sqrt', 'log2']\n",
    "\n",
    "cost_lower_sv = 0.01\n",
    "cost_upper_sv = 10.0\n",
    "kernel_sv = ['rbf', 'sigmoid']\n",
    "gamma_lower_sv = 0.001\n",
    "gamma_upper_sv = 6.5\n",
    "\n",
    "n_estimators_lower_gb = 5\n",
    "n_estimators_upper_gb = 100\n",
    "max_depth_lower_gb = 1\n",
    "max_depth_upper_gb = 8\n",
    "min_samples_split_lower_gb = 1\n",
    "min_samples_split_upper_gb = 10\n",
    "min_samples_leaf_lower_gb = 1\n",
    "min_samples_leaf_upper_gb = 5\n",
    "learning_rate_lower_gb = 0.05\n",
    "learning_rate_upper_gb = 0.5\n",
    "subsample_lower_gb = 0.75\n",
    "subsample_upper_gb = 1.0\n",
    "\n",
    "max_depth_lower_dt = 1\n",
    "max_depth_upper_dt = 10\n",
    "min_samples_split_lower_dt = 1\n",
    "min_samples_split_upper_dt = 5\n",
    "min_samples_leaf_lower_dt = 1\n",
    "min_samples_leaf_upper_dt = 10\n",
    "max_features_dt = ['sqrt', 'log2']\n",
    "\n",
    "n_neighbors_lower = 1\n",
    "n_neighbors_upper = 20\n",
    "weights =  ['uniform', 'distance']\n",
    "p = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../Dane/Oryginalne/artificial_train.data\", sep=\" \", header=None).iloc[:,:-1]\n",
    "y_train = pd.read_csv(\"../Dane/Oryginalne/artificial_train_labels.data\", sep=' ', header=None).squeeze(axis=1)\n",
    "X_test = pd.read_csv(\"../Dane/Oryginalne/artificial_test.data\", sep=\" \", header=None).iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ gridsearch miał problem odczytywaniem transformerów z innego pliku .ipynb zdecydowano się zdefiniować transfromery w pliku pythonowym. Transformery są zgodne z poczynionymi rozważeniami w pliku preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    OryginalData(),\n",
    "    PCATransformer(0.3),\n",
    "    PCATransformer(0.4),\n",
    "    PCATransformer(0.6),\n",
    "    PCATransformer(0.8),\n",
    "    CORRTransformer(0.05),\n",
    "    CORRTransformer(0.075),\n",
    "    CORRTransformer(0.7),\n",
    "    LassoSelector(),\n",
    "    RFCSelector(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etap 1\n",
    "Poniżej znajduje kod pozwalający uzyskać przesłanki do wyboru odpowiedniego stacka algorytmów uczenia maszynowego. Ponieważ zbiór danych jest dość mały to zdecydowano się na cv = 7. Ponadto wybrano hiperparametry w taki sposób aby zminimalizować ryzyko overfittingu, a żeby oddać realnie potencjał znajdujący się w poszczególnych kombinacjach algorytmów uczenia maszynowego oraz metodzie preprocessingu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h 15min\n",
    "model_list_1 = [\n",
    "    ('rf1', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb1', GradientBoostingClassifier(n_estimators=15, max_depth=2,learning_rate=0.1, random_state=42)),\n",
    "    ('sv1', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('dt1', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp1',MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "model_list_2 = [\n",
    "    ('rf2', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb2', GradientBoostingClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('sv2', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn2', KNeighborsClassifier(n_neighbors=5, )),\n",
    "    ('dt2', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp2', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "all_model_combinations = list(product(model_list_1, model_list_2))\n",
    "formatted_model_combinations = [[(name1, model1), (name2, model2)] for ((name1, model1), (name2, model2)) in all_model_combinations]\n",
    "\n",
    "for transformer in transformers:\n",
    "    stack = StackingClassifier(estimators=[], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'stack__estimators':formatted_model_combinations\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap1/{type(transformer).__name__}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisys1_OryginalDataTransformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/OryginalData.csv')\n",
    "analisys1_PCA03Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/PCA03Transformer.csv')\n",
    "analisys1_PCA04Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/PCA04Transformer.csv')\n",
    "analisys1_PCA06Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/PCA06Transformer.csv')\n",
    "analisys1_PCA08Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/PCA08Transformer.csv')\n",
    "analisys1_CORR005Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/CORR005Transformer.csv')\n",
    "analisys1_CORR075Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/CORR075Transformer.csv')\n",
    "analisys1_CORR7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/CORR7Transformer.csv')\n",
    "analisys1_LassoTransformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/LassoSelector.csv')\n",
    "analisys1_RFCTransformer = pd.read_csv('../Wyniki/ModelSearch/Etap1/RFCSelector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\"OryginalDataTransformer\", \"PCA03Transformer\", \"PCA04Transformer\",\n",
    "                \"PCA06Transformer\", \"PCA08Transformer\", \"CORR005Transformer\",\n",
    "                \"CORR075Transformer\", \"CORR7Transformer\", \"LassoTransformer\",\n",
    "                \"RFCTransformer\"]\n",
    "\n",
    "analisys_list = [analisys1_OryginalDataTransformer, analisys1_PCA03Transformer, analisys1_PCA04Transformer,\n",
    "                 analisys1_PCA06Transformer, analisys1_PCA08Transformer, analisys1_CORR005Transformer,\n",
    "                 analisys1_CORR075Transformer, analisys1_CORR7Transformer, analisys1_LassoTransformer,\n",
    "                 analisys1_RFCTransformer]\n",
    "\n",
    "print_results(transformers, analisys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższe informacje miały na celu pokazać, które metody preprocessingu zawiodły. Stąd też w dalszych rozważaniach nie będziemy analizować PCA08Transformer(), CORR005Transformer(), CORR075Transformer(), CORR7Transformer(), LassoSelector(). Widać, że najlepsze rezultaty uzykano dla PCA04Transformer(), PCA03Transformer() i RFCSelector()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best_results(analisys1_PCA03Transformer, \"PCA03Transformer\")\n",
    "print_best_results(analisys1_PCA04Transformer, \"PCA04Transformer\")\n",
    "print_best_results(analisys1_RFCTransformer, \"RFCTransformer\")\n",
    "print_best_results(analisys1_OryginalDataTransformer, \"OryginalDataTransformer\")\n",
    "print_best_results(analisys1_PCA06Transformer, \"PCA06Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z powyższych wyników dla 7 najlepszych wytrenowanych stacków można wysnuć następujące wnioski dla danych metod preprocessingu:\n",
    "\n",
    "bests_PCA03Transformer:\n",
    "- KNeighborsClassifier pojawił się we wszystkich stackach\n",
    "- Najlepszy wynik uzyskano dla KNeighborsClassifier i SVC\n",
    "\n",
    "bests_PCA04Transformer:\n",
    "- SVC pojawił się we wszystkich stackach\n",
    "- Najlepszy wynik uzyskano dla GradientBoostingClassifier i SVC\n",
    "\n",
    "bests_RFCTransformer:\n",
    "- KNeighborsClassifier pojawił się we wszystkich stackach\n",
    "- Najlepszy wynik uzyskano dla KNeighborsClassifier i SVC\n",
    "\n",
    "bests_OryginalDataTransformer:\n",
    "- KNeighborsClassifier pojawił się we wszystkich stackach\n",
    "- Najlepszy wynik uzyskano dla GradientBoostingClassifier i KNeighborsClassifier\n",
    "\n",
    "bests_PCA06Transformer:\n",
    "- SVC pojawił się we wszystkich stackach\n",
    "- Najlepszy wynik uzyskano dla GradientBoostingClassifier i SVC\n",
    "\n",
    "Różnice między wynikami konkretnych modeli w ramach jednego pliku są znikome. \n",
    "Biorąc pod uwagę powyższe wnioski i wyniki modelów z użyciem PCA zdecydowano się na ponownie przeprowadzenie wyszukiwania odpowiednego stacku, ale tym razem z mniejszą siatką modeli uczenia maszynowego i innymi transformatorami\n",
    "\n",
    "Postanowiono znaleźć lokalne maksimum dla metody PCA: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTransformers = [\n",
    "    PCATransformer(0.3),\n",
    "    PCATransformer(0.4),\n",
    "    RFCSelector(),\n",
    "    PCATransformer(9),\n",
    "    PCATransformer(8),\n",
    "    PCATransformer(7),\n",
    "    PCATransformer(6),\n",
    "    PCATransformer(5),\n",
    "    PCATransformer(4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25min\n",
    "model_list_1 = [\n",
    "    ('sv1', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=5)),\n",
    "]\n",
    "\n",
    "model_list_2 = [\n",
    "    ('rf2', RandomForestClassifier(n_estimators=25, max_depth=2, random_state=42)),\n",
    "    ('gb2', GradientBoostingClassifier(n_estimators=25, max_depth=2, random_state=42)),\n",
    "    ('sv2', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn2', KNeighborsClassifier(n_neighbors=5, )),\n",
    "    ('dt2', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "]\n",
    "\n",
    "all_model_combinations = list(product(model_list_1, model_list_2))\n",
    "formatted_model_combinations = [[(name1, model1), (name2, model2)] for ((name1, model1), (name2, model2)) in all_model_combinations]\n",
    "\n",
    "for transformer in newTransformers:\n",
    "    stack = StackingClassifier(estimators=[], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'stack__estimators':formatted_model_combinations\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap2/{type(transformer).__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisys2_PCA4Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA4Transformer.csv')\n",
    "analisys2_PCA5Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA5Transformer.csv')\n",
    "analisys2_PCA6Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA6Transformer.csv')\n",
    "analisys2_PCA7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA7Transformer.csv')\n",
    "analisys2_PCA8Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA8Transformer.csv')\n",
    "analisys2_PCA9Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA9Transformer.csv')\n",
    "analisys2_PCA03Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA03Transformer.csv')\n",
    "analisys2_PCA04Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/PCA04Transformer.csv')\n",
    "analisys2_RFCTransformer = pd.read_csv('../Wyniki/ModelSearch/Etap2/RFCSelector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\"RFCTransformer\", \"PCA04Transformer\", \"PCA03Transformer\",\n",
    "                \"PCA9Transformer\", \"PCA8Transformer\", \"PCA7Transformer\",\n",
    "                \"PCA6Transformer\", \"PCA5Transformer\", \"PCA4Transformer\"]\n",
    "\n",
    "analisys_list = [analisys2_RFCTransformer, analisys2_PCA04Transformer, analisys2_PCA03Transformer,\n",
    "                 analisys2_PCA9Transformer, analisys2_PCA8Transformer, analisys2_PCA7Transformer,\n",
    "                 analisys2_PCA6Transformer, analisys2_PCA5Transformer, analisys2_PCA4Transformer]\n",
    "\n",
    "print_results(transformers, analisys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że najlepsze wyniki oraz średnio najlepsze wyniki uzyskano dla PCA5 oraz PCA7. Dlatego w dalszych rozważaniach tylko te metody będą brane pod uwagę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best_results(analisys2_PCA03Transformer, \"PCA03Transformer\")\n",
    "print_best_results(analisys2_PCA04Transformer, \"PCA04Transformer\")\n",
    "print_best_results(analisys2_RFCTransformer, \"RFCTransformer\")\n",
    "print_best_results(analisys2_PCA5Transformer, \"PCA5Transformer\")\n",
    "print_best_results(analisys2_PCA7Transformer, \"PCA7Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki nie różnią się znacząco od poprzednich. Dla\n",
    "Na podstawie powyższych wyników zdecydowano się tunować następujące algorytmy: \n",
    "- KNeighborsClassifier z SVC\n",
    "- KNeighborsClassifier z RandomForestClassifier\n",
    "- KNeighborsClassifier z GradientBoostingClassifier\n",
    "- KNeighborsClassifier z DecisionTreeClassifier\n",
    "\n",
    "i metody preprocessingu PCA5 i PCA7 \n",
    "Poniżej wykonano tunowanie omawianych modeli. \n",
    "\n",
    "Spodziewane wyniki to przewaga modelu KNeighborsClassifier z SVC po tuningu nad resztą algorytmów. Przewaga PCA5 nad PCA7 we wszystkich przypadkach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etap 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTransformers = [\n",
    "    PCATransformer(5),\n",
    "    PCATransformer(7),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h 15min \n",
    "for transformer in finalTransformers:\n",
    "    stack = StackingClassifier(estimators=[ \n",
    "        ('kn1', KNeighborsClassifier()),\n",
    "        ('rf1', RandomForestClassifier(random_state=random_state)),\n",
    "        ], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "    'model__rf1__n_estimators': np.linspace(n_estimators_lower_rf, n_estimators_upper_rf, n_estimators_upper_rf - n_estimators_lower_rf).astype(int),\n",
    "    'model__rf1__max_depth': np.linspace(max_depth_lower_rf, max_depth_upper_rf, max_depth_upper_rf- max_depth_lower_rf).astype(int),\n",
    "    'model__rf1__min_samples_split': np.linspace(min_samples_split_lower_rf, min_samples_split_upper_rf, min_samples_split_upper_rf - min_samples_split_lower_rf).astype(int),\n",
    "    'model__rf1__min_samples_leaf': np.linspace(min_samples_leaf_lower_rf, min_samples_leaf_upper_rf, min_samples_leaf_upper_rf - min_samples_leaf_lower_rf).astype(int),\n",
    "    'model__rf1__max_features': max_features_rf,\n",
    "    'model__kn1__n_neighbors': np.linspace(n_neighbors_lower, n_neighbors_upper, n_neighbors_upper - n_neighbors_lower).astype(int),\n",
    "    'model__kn1__weights': weights,\n",
    "    'model__kn1__p': p\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch_rf,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(random_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap3/KNRF{type(transformer).__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h 10min \n",
    "for transformer in finalTransformers:\n",
    "    stack = StackingClassifier(estimators=[ \n",
    "        ('kn1', KNeighborsClassifier()),\n",
    "        ('sv1', SVC(random_state=random_state)),\n",
    "        ], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "    'model__sv1__C': np.linspace(cost_lower_sv, cost_upper_sv, numberOfPoints).astype(float),\n",
    "    'model__sv1__kernel': kernel_sv,\n",
    "    'model__sv1__gamma':  np.linspace(gamma_lower_sv, gamma_upper_sv, numberOfPoints).astype(float),\n",
    "    'model__kn1__n_neighbors': np.linspace(n_neighbors_lower, n_neighbors_upper, n_neighbors_upper - n_neighbors_lower).astype(int),\n",
    "    'model__kn1__weights': weights,\n",
    "    'model__kn1__p': p\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch_sv,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(random_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap3/KNSV{type(transformer).__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h 45min \n",
    "for transformer in finalTransformers:\n",
    "    stack = StackingClassifier(estimators=[ \n",
    "        ('kn1', KNeighborsClassifier()),\n",
    "        ('gb1', GradientBoostingClassifier(random_state=random_state)),\n",
    "        ], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "    'model__gb1__n_estimators': np.linspace(n_estimators_lower_gb, n_estimators_upper_gb, n_estimators_upper_gb - n_estimators_lower_gb).astype(int),\n",
    "    'model__gb1__max_depth': np.linspace(max_depth_lower_gb, max_depth_upper_gb, max_depth_upper_gb- max_depth_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_split': np.linspace(min_samples_split_lower_gb, min_samples_split_upper_gb, min_samples_split_upper_gb - min_samples_split_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_leaf': np.linspace(min_samples_leaf_lower_gb, min_samples_leaf_upper_gb, min_samples_leaf_upper_gb - min_samples_leaf_lower_gb).astype(int),\n",
    "    'model__gb1__learning_rate': np.linspace(learning_rate_lower_gb, learning_rate_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__gb1__subsample': np.linspace(subsample_lower_gb, subsample_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__kn1__n_neighbors': np.linspace(n_neighbors_lower, n_neighbors_upper, n_neighbors_upper - n_neighbors_lower).astype(int),\n",
    "    'model__kn1__weights': weights,\n",
    "    'model__kn1__p': p\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch_gb,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(random_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap3/KNGB{type(transformer).__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h 30min \n",
    "for transformer in finalTransformers:\n",
    "    stack = StackingClassifier(estimators=[ \n",
    "        ('kn1', KNeighborsClassifier()),\n",
    "        ('dt1', DecisionTreeClassifier(random_state=random_state)),\n",
    "        ], final_estimator=LogisticRegression())\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', transformer),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "    'model__dt1__max_depth': np.linspace(max_depth_lower_dt, max_depth_upper_dt, max_depth_upper_dt- max_depth_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_split': np.linspace(min_samples_split_lower_dt, min_samples_split_upper_dt, min_samples_split_upper_dt - min_samples_split_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_leaf': np.linspace(min_samples_leaf_lower_dt, min_samples_leaf_upper_dt, min_samples_leaf_upper_dt - min_samples_leaf_lower_dt).astype(int),\n",
    "    'model__dt1__max_features': max_features_dt,\n",
    "    'model__kn1__n_neighbors': np.linspace(n_neighbors_lower, n_neighbors_upper, n_neighbors_upper - n_neighbors_lower).astype(int),\n",
    "    'model__kn1__weights': weights,\n",
    "    'model__kn1__p': p\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch_dt,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    cs_results = pd.DataFrame(random_search.cv_results_)\n",
    "    save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap3/KNDT{type(transformer).__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisys3_KN_DT_PCA5Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNDTPCA5Transformer.csv')\n",
    "analisys3_KN_GB_PCA5Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNGBPCA5Transformer.csv')\n",
    "analisys3_KN_RF_PCA5Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNRFPCA5Transformer.csv')\n",
    "analisys3_KN_SV_PCA5Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNSVPCA5Transformer.csv')\n",
    "analisys3_KN_DT_PCA7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNDTPCA7Transformer.csv')\n",
    "analisys3_KN_GB_PCA7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNGBPCA7Transformer.csv')\n",
    "analisys3_KN_RF_PCA7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNRFPCA7Transformer.csv')\n",
    "analisys3_KN_SV_PCA7Transformer = pd.read_csv('../Wyniki/ModelSearch/Etap3/KNSVPCA7Transformer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\"KN_DT_PCA5Transformer\", \"KN_GB_PCA5Transformer\", \"KN_RF_PCA5Transformer\",\n",
    "                \"KN_SV_PCA5Transformer\", \"KN_DT_PCA7Transformer\", \"KN_GB_PCA7Transformer\",\n",
    "                \"KN_RF_PCA7Transformer\", \"KN_SV_PCA7Transformer\"]\n",
    "\n",
    "analisys_list = [analisys3_KN_DT_PCA5Transformer, analisys3_KN_GB_PCA5Transformer, analisys3_KN_RF_PCA5Transformer,\n",
    "                 analisys3_KN_SV_PCA5Transformer, analisys3_KN_DT_PCA7Transformer, analisys3_KN_GB_PCA7Transformer,\n",
    "                 analisys3_KN_RF_PCA7Transformer, analisys3_KN_SV_PCA7Transformer]\n",
    "\n",
    "print_results(transformers, analisys_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(searchMaxRowByColumn(analisys3_KN_GB_PCA5Transformer,'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że przewidywania co do przewagi modelu z SVM okazały się błędne, a najlepszym modelem okazał się stack zawierający KNeighborsClassifier i GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etap 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy uzyskać jeszcze lepsze wyniki na najlepszym stacku poprzez wylosowanie w metodzie randomSearch 5 razy więcej punktów niż poprzednio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3h 45min \n",
    "stack = StackingClassifier(estimators=[ \n",
    "    ('kn1', KNeighborsClassifier()),\n",
    "    ('gb1', GradientBoostingClassifier(random_state=random_state)),\n",
    "    ], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessing', PCA5Transformer()),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "param_dist = {\n",
    "    'model__gb1__n_estimators': np.linspace(n_estimators_lower_gb, n_estimators_upper_gb, n_estimators_upper_gb - n_estimators_lower_gb).astype(int),\n",
    "    'model__gb1__max_depth': np.linspace(max_depth_lower_gb, max_depth_upper_gb, max_depth_upper_gb- max_depth_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_split': np.linspace(min_samples_split_lower_gb, min_samples_split_upper_gb, min_samples_split_upper_gb - min_samples_split_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_leaf': np.linspace(min_samples_leaf_lower_gb, min_samples_leaf_upper_gb, min_samples_leaf_upper_gb - min_samples_leaf_lower_gb).astype(int),\n",
    "    'model__gb1__learning_rate': np.linspace(learning_rate_lower_gb, learning_rate_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__gb1__subsample': np.linspace(subsample_lower_gb, subsample_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__kn1__n_neighbors': np.linspace(n_neighbors_lower, n_neighbors_upper, n_neighbors_upper - n_neighbors_lower).astype(int),\n",
    "    'model__kn1__weights': weights,\n",
    "    'model__kn1__p': p\n",
    "    }\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch_final,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "cs_results = pd.DataFrame(random_search.cv_results_)\n",
    "save_data_to_csv(cs_results, f\"../Wyniki/ModelSearch/Etap4/FianlModel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisys_final_best = pd.read_csv('../Wyniki/ModelSearch/Etap4/FianlModel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNajlepszy znaleziony model:\\n\")\n",
    "print(searchMaxRowByColumn(analisys_final_best,'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety nie widać aż tak dużej poprawy względem tego co uzyskano pod koniec etapu 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza końcowa i trening finałowych modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening następujących stacków do analizy końcowej: \n",
    "- KNeighborsClassifier z SVC z 3 etapu \n",
    "- KNeighborsClassifier z RandomForestClassifier z 3 etapu \n",
    "- KNeighborsClassifier z GradientBoostingClassifier z 4 etapu \n",
    "- KNeighborsClassifier z DecisionTreeClassifier z 3 etapu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best result: 0.88948, sprawdzarka: 0.9667\n",
    "stack = StackingClassifier(estimators=[ ('dt1', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=3, max_features='sqrt', max_depth=6, random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=4, p=2 , weights='distance'))], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessing', PCA5Transformer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "proba_dt = pd.DataFrame(proba).iloc[:,1:]\n",
    "\n",
    "save_prediction_to_file(proba_dt, '../Wyniki/Predykcje/MANUAL/KN_DT_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best result: 0.895988, sprawdzarka: 0.9333\n",
    "stack = StackingClassifier(estimators=[ ('gb1', GradientBoostingClassifier( subsample=0.833333333333333, n_estimators=91,  min_samples_split=8, min_samples_leaf=3, learning_rate=0.21875, max_depth=8, random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=3, p=2 , weights='distance'))], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessing', PCA5Transformer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "proba_gb = pd.DataFrame(proba).iloc[:,1:]\n",
    "\n",
    "save_prediction_to_file(proba_gb, '../Wyniki/Predykcje/MANUAL/KN_GB_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best result: 0.88849, sprawdzarka: 0.9333\n",
    "stack = StackingClassifier(estimators=[ ('rf1', RandomForestClassifier( max_features='sqrt', n_estimators=36,  min_samples_split=3, min_samples_leaf=5, max_depth=8, random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=7, p=2 , weights='distance'))], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessing', PCA5Transformer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "proba_rf = pd.DataFrame(proba).iloc[:,1:]\n",
    "\n",
    "save_prediction_to_file(proba_rf, '../Wyniki/Predykcje/MANUAL/KN_RF_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best result: 0.888488, sprawdzarka: nie było czasu aby sprawdzić, ale ten wynik na sprawdzarce by nic nie zmienił \n",
    "stack = StackingClassifier(estimators=[ ('sv1', SVC(kernel='rbf', gamma=6.5, C=1.25875, random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=4, p=2 , weights='distance'))], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessing', PCA5Transformer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "proba_sv = pd.DataFrame(proba).iloc[:,1:]\n",
    "\n",
    "save_prediction_to_file(proba_sv, '../Wyniki/Predykcje/MANUAL/KN_SV_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej widczony jest rozkład zmiennej odpowiedzi w zależności od modelu. Najbardziej symetryczny rozkład występuję dla proba_gb. Dlatego też wybieramy ten model jako nasz najlepszy, pomimo tego, że w sprawdzarce uzyskał 2 najlepszy wynik  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "proba_dt.plot(kind='hist', bins=50, alpha=0.5)\n",
    "plt.title('Zmienna odpowiedzi dla stacka zawierającego algorytm DecisionTreeClassifier')\n",
    "plt.xlabel('Wartość')\n",
    "plt.ylabel('Liczba Obserwacji')\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"proba_dt\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "proba_gb.plot(kind='hist', bins=50, alpha=0.5)\n",
    "plt.title('Zmienna odpowiedzi dla stacka zawierającego algorytm GradientBoostingClassifier')\n",
    "plt.xlabel('Wartość')\n",
    "plt.ylabel('Liczba Obserwacji')\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"proba_gb\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "proba_rf.plot(kind='hist', bins=50, alpha=0.5)\n",
    "plt.title('Zmienna odpowiedzi dla stacka zawierającego algorytm RandomForestClassifier')\n",
    "plt.xlabel('Wartość')\n",
    "plt.ylabel('Liczba Obserwacji')\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"proba_rf\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "proba_sv.plot(kind='hist', bins=50, alpha=0.5)\n",
    "plt.title('Zmienna odpowiedzi dla stacka zawierającego algorytm SVM')\n",
    "plt.xlabel('Wartość')\n",
    "plt.ylabel('Liczba Obserwacji')\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"proba_sv\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
