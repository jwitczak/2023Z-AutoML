{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym dokumencie zostały poruszone następujące kwestie: \n",
    "1. Podział danych \n",
    "2. Przygotowanie danych\n",
    "3. Analiza ekploracyjna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podział danych \n",
    "Zdecydowano się na nie przeprowadzanie podziału danych na zbiór testowy i treningowy z pliku artificial_train.data. Podjęto tą decyzję w oparciu na ograniczoną liczbę rekordów, co zwiększało by szanse na ominięcie rekordów niosących istotną informację. Podstawową formą oceny modelu staje się więc mechanizm cross walidacji. Aby zniwelować negatywy płynące z wielkości zbioru danych przyjęto kross walidacje jako sposób oceny modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../Dane/Oryginalne/artificial_train.data\", sep=\" \", header=None).iloc[:,:-1]\n",
    "y_train = pd.read_csv(\"../Dane/Oryginalne/artificial_train_labels.data\", sep=' ', header=None)\n",
    "X_test = pd.read_csv(\"../Dane/Oryginalne/artificial_test.data\", sep=\" \", header=None).iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych oraz analiza ekploracyjna\n",
    "Spojrzenie na dane w ujęciu statystycznym i sprawdzenie znaczących fatków "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzenie braków danych \n",
    "print(X_train.isnull().sum())\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzenie rozkładu zmiennej Y \n",
    "counterOne = (y_train == 1).sum().sum()\n",
    "counterMinusOne = (y_train == -1).sum().sum()\n",
    "\n",
    "print(\"Liczba wystąpień 1:\", counterOne)\n",
    "print(\"Liczba wystąpień -1:\", counterMinusOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzenie występowania duplikatów  \n",
    "duplicatedRows = X_train[X_train.duplicated()]\n",
    "print(\"Liczba duplikatów:\", len(duplicatedRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że dane nie mają braków a rozkład zmiennej odpowiedzi jest idealny w badanym przypadku. Ponadto żaden rekord nie jest zduplikowany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxploty dla każdej kolumny w X_train\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=X_train)\n",
    "plt.xticks(ticks=range(0, X_train.shape[1], 25), labels=range(0, X_train.shape[1], 25))\n",
    "plt.title(\"Boxploty dla kolumn w X_train przed oczyszczeniem nadmiarowych cech\")\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"oryginal\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Większość cech ma podobny rozkład z tego samego zakresu wartości. Skłania nas to do wniosku, że dane mogą być silnie skorelowane. \n",
    "\n",
    "W związku z tym należy rozważyć przekształcenie zbioru danych pod kątem: \n",
    "- odrzucenie rekordów z obserwacjami odstającymi, ponieważ mogą być tylko szumem informacyjnym (ale mogą się również okazać niezwykle inforamtywne)\n",
    "- redukcja wymiarów poprzez wykrycie kolumn skorelowanych (kolumny skorelowane nie wnoszą kluczowych informacji)\n",
    "\n",
    "Zauważmy również, że dane składają się jedyne z liczb naturalnych o podobnym rozkładzie co sprawia, że normalizacja i standaryzacja nie ma większego uzasadnienia. Z natury zmiennych nie potrzebne jest kodowanie zmiennych kategorycznych oraz nie możliwe jest tworzenie cech interakcyjnych, ponieważ nie mamy informacji o znaczeniu pragmatycznym zmiennych. \n",
    "\n",
    "Należy wziąć pod uwagę, że ratio cech do rekordów wynosi 1:4 co nie jest według nas wystarczająco zadawalające. Dlatego też będziemy dążyć do ograniczenia eksperymentalnie liczby cech, aby uniknąć łatwo możliwego przetrenowania modelu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbadajmy jeszcze skośność danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzenie skośności danych\n",
    "skewOfX = X_train.skew()\n",
    "print(skewOfX.min())\n",
    "print(skewOfX.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartości od -0.13 do 0.18 sugerują, że dane mają umiarkowanie symetryczny rozkład, więc nie powinno to wpływać na skuteczność działania niektórych modeli uczenia maszynowego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz korzystania z oryginalnego zestawu danych zdecydowano się na następujące przekształcenia zbioru treningowego i testowego:\n",
    "- PCA - 0.3, 0.4, 0.6, 0.8 - wartość określa ile wariancji w ujęciu całościowym chcemy zachować w danych\n",
    "- Własnoręczne usuwanie korelacji używając macierzy koleracji przy współczynnikach korelacji - 0.7, 0.075, 0.05\n",
    "- wykorzystanie cech istotnych dla modelu regresji liniowej \n",
    "- wykorzystanie cech istotnych dla modelu random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA - Analiza Głównych Składowych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Liczba komponentów')\n",
    "plt.ylabel('Kumulatywna wariancja')\n",
    "plt.title('Analiza łokcia dla PCA używająć kumulatywnej wariancji')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Liczba komponentów')\n",
    "plt.ylabel('Wariancja')\n",
    "plt.title('Analiza łokcia dla PCA')\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"anlce\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że wszystkie kolejne cechy wnoszą niewiele do skomulowanej wariancji, a łokieć jest dość wyraźny. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca_with_mle(X_train, X_test):\n",
    "    pca = PCA(n_components='mle', random_state=42)\n",
    "    X_train_transformed = pca.fit_transform(X_train)\n",
    "    X_test_transformed = pca.transform(X_test)\n",
    "    return X_train_transformed, X_test_transformed\n",
    "\n",
    "X_train_MLE, X_test_MLE = apply_pca_with_mle(X_train,X_test)\n",
    "print(X_train_MLE.shape)\n",
    "print(X_test_MLE.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatyczne redukowane zmiennych nie działa dostatecznie dobrze, bo zostały zredukowane tylko 2 cechy. Dlatego postanowiono pominąć ten sposób przekształcenia. Dodatkowym powodem na ominiecie tej metody bylo dlugość obliczeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca_with_var(X_train, n_components, random_state):\n",
    "    pca = PCA(n_components=n_components, random_state=random_state)\n",
    "    X_train_transformed = pca.fit_transform(X_train)\n",
    "    return X_train_transformed\n",
    "\n",
    "randomState = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3  = apply_pca_with_var(X_train,0.3,randomState)\n",
    "print(X_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4 = apply_pca_with_var(X_train,0.4,randomState)\n",
    "print(X_train_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6 = apply_pca_with_var(X_train,0.6,randomState)\n",
    "print(X_train_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_8 = apply_pca_with_var(X_train,0.8,randomState)\n",
    "print(X_train_8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macierz korelacji\n",
    "Zgodnie z dokumentacją agorytm PCA w pierwszej kolejności odrzuca zmienne wysokoskorelowane więc nie ma uzasadnienia badanie korelacji wynikowych dataframe'ów uzysanych pooprzez stosowanie algorytmu PCA. Zbudujmy dataset ręcznie używając macierzy korelacji.\n",
    "\n",
    "wartość 0.7 jest standardową wartością z literatury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "def remove_correlated_features(X_train, correlation_matrix, threshold):\n",
    "    correlated_vars = np.where(np.abs(correlation_matrix) > threshold)\n",
    "    features_to_remove = []\n",
    "    for var1, var2 in zip(*correlated_vars):\n",
    "        if var1 != var2 and var1 < var2:\n",
    "            features_to_remove.append(var2)\n",
    "\n",
    "    features_to_remove = list(set(features_to_remove))\n",
    "    print(f'Cechy do usunięcia: {features_to_remove}')\n",
    "    X_train_filtered = X_train.drop(columns=X_train.columns[features_to_remove], inplace=False)\n",
    "    print(f'Kształt X_train po usunięciu cech: {X_train_filtered.shape}')\n",
    "    return X_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_C7 = remove_correlated_features(X_train, correlation_matrix, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_C1 = remove_correlated_features(X_train, correlation_matrix, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że mało jest danych silnie skorelowanych co może pokazywać, że model nie będzie źle dziłać nawet używając całego zbioru danych, bez redukcji liczby kolumn. W celach eksperymentalno - edukacyjnym możemy przetestować co się stanie z modelami gdy zastosujemy zmienne skorelowanie na poziomie 0.075 i 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_C075 = remove_correlated_features(X_train, correlation_matrix, 0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_C05 = remove_correlated_features(X_train, correlation_matrix, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozyskiwanie najważniejszych cech z perspektywy algorytmu LASSO\n",
    "\n",
    "Została użyta regresja LASSO, ponieważ wprowadza regularyzację L1, co skutkuje rzadkimi wagami cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_model = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5, random_state=42)\n",
    "lasso_cv_model.fit(X_train, y_train)\n",
    "\n",
    "sel = SelectFromModel(lasso_cv_model, prefit=True)\n",
    "\n",
    "sel.fit(X_train, y_train.values.reshape(-1),random_state=42)\n",
    "\n",
    "selected_feat = X_train.columns[(sel.get_support())]\n",
    "\n",
    "X_train_LA = X_train.loc[:,selected_feat]\n",
    "X_test_LA = X_test.loc[:,selected_feat]\n",
    "\n",
    "print(X_train_LA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozyskiwanie najważniejszych cech z perspektywy algorytmu Random Forest\n",
    "\n",
    "Został użyty Random Forest, ze względu na jego zdolność do oceny ważności cech na podstawie kryterium Gini lub Entropii. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train.values.flatten())\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "sel = SelectFromModel(grid_search.best_estimator_)\n",
    "\n",
    "sel.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "selected_feat = X_train.columns[(sel.get_support())]\n",
    "len(selected_feat)\n",
    "\n",
    "X_train_RF = X_train.loc[:,selected_feat]\n",
    "X_test_RF = X_test.loc[:,selected_feat]\n",
    "print(X_test_RF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważono, że użycie powyższych algorytmu PCA wykluczyło cechy, które zawierały obserwacje odstające. Jedynie gdzie widzimy nadal obserwacje odstające to dla ręcznym usuwaniu cech na podstawie macierzy korelacji dla współczynnika 0.7. Ale uważamy, że będzie to dobry miernik istoty tych zmiennych na wyniki modelu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=X_train_8)\n",
    "plt.xticks(ticks=range(0, X_train_8.shape[1], 5), labels=range(0, X_train_8.shape[1], 5))\n",
    "plt.title(\"Boxploty dla kolumn w X_train po oczyszczeniu cech metodą PCA dla współczynnika 0.8\")\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"PCA08\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=X_train_C075)\n",
    "plt.xticks(ticks=range(0, X_train_C075.shape[1], 10), labels=range(0, X_train_C075.shape[1], 10))\n",
    "plt.title(\"Boxploty dla kolumn w X_train po oczyszczeniu cech używając macieży korelacji i współczynnika 0.7\")\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"CORR07\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=X_train_RF)\n",
    "plt.xticks(ticks=range(0, X_train_RF.shape[1], 4), labels=range(0, X_train_RF.shape[1], 4))\n",
    "plt.title(\"Boxploty dla kolumn w X_train po oczyszczeniu cech używając algorytmu RandomForest\")\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"RandomForest\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=X_train_LA)\n",
    "plt.xticks(ticks=range(0, X_train_LA.shape[1], 1), labels=range(0, X_train_LA.shape[1], 1))\n",
    "plt.title(\"Boxploty dla kolumn w X_train po oczyszczeniu cech używając algorytmu LASSO\")\n",
    "plt.savefig('../Wyniki/Wykresy/' + \"LASSO\" + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
