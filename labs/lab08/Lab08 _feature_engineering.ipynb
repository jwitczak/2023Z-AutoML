{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFz_7s2p4bHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering in AutoML"
      ],
      "metadata": {
        "id": "TTS3-vAh4dxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 1\n",
        "\n",
        "Na podstawie dopasowanych modeli w autosklearnie (w wersji 1.0 i 2.0) zrób zestawienie technik feature preprocessing wykorzystywanych w tym pakiecie. Aby zrobić to zestawienie w sposób systematyczny i powtarzalny można zapisywać otrzymane wyniki do listy lub słownika, który potem możemy analizować. Przetestujcie Autosklearn dla 5 różnych zbiorów danych.\n",
        "\n",
        "Jeśli nie znasz jakiejś metody preprocessingu - to jest świetna okazja żeby zajrzeć do dokumentacji 🔍, oprócz AutoML poszerzymy horyzonty 🌄\n",
        "\n",
        "\n",
        "Następnie odpowiedz na pytania:\n",
        "1. Jakie techniki były najczęściej używane?\n",
        "2. Czy są różnice pomiędzy Autosklearn 1.0 i 2.0? Jaki może być powód potencjalnych różnic?\n",
        "3. Czy w zależności od innych danych inne techniki preprocessingu były wybierane?\n"
      ],
      "metadata": {
        "id": "BoGBEAYw4s4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalacja Autosklearn (pamiętaj o restarcie po instalacji)"
      ],
      "metadata": {
        "id": "Y0Flqi4m5_aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. uninstall all affected packages\n",
        "# !pip uninstall -y Cython scipy pyparsing scikit_learn imbalanced-learn mlxtend yellowbrick\n",
        "\n",
        "# # 2. install packages to be downgraded\n",
        "# !pip install Cython==0.29.36 scipy==1.9 pyparsing==2.4\n",
        "\n",
        "# # 3. install older scikit-learn disregarding its dependencies\n",
        "# !pip install scikit-learn==0.24.2 --no-build-isolation\n",
        "\n",
        "# # 4. finally install auto-sklearn\n",
        "# !pip install auto-sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qMhz8Ev_rPH_",
        "outputId": "a9f5a6b6-253f-4fd0-bf66-e8e9bfffd026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Cython 3.0.5\n",
            "Uninstalling Cython-3.0.5:\n",
            "  Successfully uninstalled Cython-3.0.5\n",
            "Found existing installation: scipy 1.11.3\n",
            "Uninstalling scipy-1.11.3:\n",
            "  Successfully uninstalled scipy-1.11.3\n",
            "Found existing installation: pyparsing 3.1.1\n",
            "Uninstalling pyparsing-3.1.1:\n",
            "  Successfully uninstalled pyparsing-3.1.1\n",
            "Found existing installation: scikit-learn 1.2.2\n",
            "Uninstalling scikit-learn-1.2.2:\n",
            "  Successfully uninstalled scikit-learn-1.2.2\n",
            "Found existing installation: imbalanced-learn 0.10.1\n",
            "Uninstalling imbalanced-learn-0.10.1:\n",
            "  Successfully uninstalled imbalanced-learn-0.10.1\n",
            "Found existing installation: mlxtend 0.22.0\n",
            "Uninstalling mlxtend-0.22.0:\n",
            "  Successfully uninstalled mlxtend-0.22.0\n",
            "Found existing installation: yellowbrick 1.5\n",
            "Uninstalling yellowbrick-1.5:\n",
            "  Successfully uninstalled yellowbrick-1.5\n",
            "Collecting Cython==0.29.36\n",
            "  Downloading Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.9\n",
            "  Downloading scipy-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing==2.4\n",
            "  Downloading pyparsing-2.4.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.25.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.9) (1.23.5)\n",
            "Installing collected packages: scipy, pyparsing, Cython\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.13 requires scikit-learn, which is not installed.\n",
            "librosa 0.10.1 requires scikit-learn>=0.20.0, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\", but you have pyparsing 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Cython-0.29.36 pyparsing-2.4.0 scipy-1.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (3.2.0)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-0.24.2-cp310-cp310-linux_x86_64.whl size=22231991 sha256=05e2f14c2076d989115c195f77783973bbf49feb3bfbd1f75f26df74dbbcf450\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/a4/68/4e78865652fa14db4a162b491e5138565f97646f9e1f2ab8cc\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.13.0 requires scikit-learn>=1.2.2, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-0.24.2\n",
            "Collecting auto-sklearn\n",
            "  Downloading auto-sklearn-0.15.0.tar.gz (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (4.5.0)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: dask>=2021.12 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2012.12 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2023.8.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (6.0.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.5.3)\n",
            "Collecting liac-arff (from auto-sklearn)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (3.2.0)\n",
            "Collecting ConfigSpace<0.5,>=0.4.21 (from auto-sklearn)\n",
            "  Downloading ConfigSpace-0.4.21-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynisher<0.7,>=0.6.3 (from auto-sklearn)\n",
            "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyrfr<0.9,>=0.8.1 (from auto-sklearn)\n",
            "  Downloading pyrfr-0.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smac<1.3,>=1.2 (from auto-sklearn)\n",
            "  Downloading smac-1.2.tar.gz (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (0.29.36)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (1.4.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (6.8.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.1.2)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (6.3.2)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->auto-sklearn) (2023.3.post1)\n",
            "Collecting emcee>=3.0.0 (from smac<1.3,>=1.2->auto-sklearn)\n",
            "  Downloading emcee-3.1.4-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2021.12->auto-sklearn) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2012.12->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0->auto-sklearn) (1.16.0)\n",
            "Building wheels for collected packages: auto-sklearn, pynisher, smac, liac-arff\n",
            "  Building wheel for auto-sklearn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sklearn: filename=auto_sklearn-0.15.0-py3-none-any.whl size=6641936 sha256=8f6c219953bec4775945f4b16e5930dc9cdbfbb4e4900de715cc2611343b1aaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/0a/f9/8c1a06bcc36bc16b467b044b5bb03a90f92a5c5e6cd443414b\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7026 sha256=e73383abb4685314abdca32574432e1469a01b0483b5663293687c7944bab387\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/7b/53/b21d6b41910f43c7f1557262e579598f83e75e44c659c1bcce\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smac: filename=smac-1.2-py3-none-any.whl size=215906 sha256=5126dd380b9d64a015178f3c15e132c7beb26e015ebfd6fe2baedcceb91a03d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/2e/d9/2db14bdfcdc36bf12e202b44201df03f194367fcfd85ce2778\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=d99ae899bfe64b72baa595f2537647d1813c57c54f733ad4846bbbc267fc6160\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\n",
            "Successfully built auto-sklearn pynisher smac liac-arff\n",
            "Installing collected packages: pyrfr, pynisher, liac-arff, emcee, ConfigSpace, smac, auto-sklearn\n",
            "Successfully installed ConfigSpace-0.4.21 auto-sklearn-0.15.0 emcee-3.1.4 liac-arff-2.5.0 pynisher-0.6.4 pyrfr-0.8.3 smac-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "770gboUV6adz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huB_rzTzq9yr",
        "outputId": "2469da79-8178-4557-b9c6-1529a6b9baa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:404: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
            "  warn(\"Multiple active versions of the dataset matching the name\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done downloading credit-g\n"
          ]
        }
      ],
      "source": [
        "from autosklearn.classification import AutoSklearnClassifier\n",
        "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
        "from autosklearn.metrics import roc_auc\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "\n",
        "# From OpenML: https://www.openml.org/d/31\n",
        "dataset_name = \"credit-g\"\n",
        "\n",
        "def get_data_and_scoring_function(dataset_name):\n",
        "    X, y = sklearn.datasets.fetch_openml(dataset_name, as_frame=True, return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
        "        X, y, random_state=42, stratify=y,\n",
        "    )\n",
        "\n",
        "    def scoring_function(estimator):\n",
        "        predictions = estimator.predict_proba(X_test)[:, 1]\n",
        "        return sklearn.metrics.roc_auc_score(y_test, predictions)\n",
        "\n",
        "    def train_scoring_function(estimator):\n",
        "        predictions = estimator.predict_proba(X_train)[:, 1]\n",
        "        return sklearn.metrics.roc_auc_score(y_train, predictions)\n",
        "\n",
        "    def get_test_data():\n",
        "        return X_test, y_test\n",
        "\n",
        "    return X_train, y_train, get_test_data, scoring_function, train_scoring_function\n",
        "\n",
        "X_train, y_train, get_test_data, scoring_function, train_scoring_function = get_data_and_scoring_function(dataset_name)\n",
        "\n",
        "print(f\"Done downloading {dataset_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trening\n"
      ],
      "metadata": {
        "id": "mk4gsxGv6Hsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = {\n",
        "  \"time_left_for_this_task\": 120,  # seconds\n",
        "  \"seed\": 42,\n",
        "  \"metric\": roc_auc,\n",
        "  \"n_jobs\": 4,\n",
        "}\n",
        "\n",
        "# This will only be used by autosklearn 1 while autosklearn 2 will automatically\n",
        "# select a strategy\n",
        "resampling_strategy = \"holdout\"\n",
        "\n",
        "#-------------------------\n"
      ],
      "metadata": {
        "id": "fHJYoSbprN-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "askl1 = AutoSklearnClassifier(\n",
        "    **settings,\n",
        "    resampling_strategy=resampling_strategy\n",
        ")\n",
        "askl1.fit(X_train, y_train, dataset_name=\"credit-g\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IujBy2q7q_Mz",
        "outputId": "cfc1b512-6659-4ec6-dbb8-ad6036999f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autosklearn/data/target_validator.py:187: UserWarning: Fitting transformer with a pandas series which has the dtype category. Inverse transform may not be able preserve dtype when converting to np.ndarray\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(ensemble_class=<class 'autosklearn.ensembles.ensemble_selection.EnsembleSelection'>,\n",
              "                      metric=roc_auc, n_jobs=4, per_run_time_limit=48, seed=42,\n",
              "                      time_left_for_this_task=120)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(f\"Auto-sklearn 1.0 | train = {train_scoring_function(askl1)} | test = {scoring_function(askl1)}\")\n",
        "print(f\"Selected `resampling-strategy` = {askl1.resampling_strategy}\")\n",
        "print(f\"Selected `resampling-strategy-arguments` = {askl1.resampling_strategy_arguments}\")\n",
        "\n",
        "# Some quick summary statistics\n",
        "print(askl1.sprint_statistics())\n",
        "\n",
        "# The leaderboard shows all the models during the optimization process,\n",
        "# see this link for arguments if you want to see more!\n",
        "# https://automl.github.io/auto-sklearn/master/api.html#autosklearn.classification.AutoSklearnClassifier.leaderboard\n",
        "leaderboard = askl1.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
        "print(leaderboard)\n",
        "\n",
        "# Show all the models in the final produced ensemble\n",
        "# pprint(askl1.show_models())\n",
        "\n",
        "# For compatibility with scikit-learn we implement `cv_results_`, but the output is pretty lengthy, so we leave this commented\n",
        "# print(askl1.cv_results_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSifD0fMrgSe",
        "outputId": "5b63d34c-95e0-4ac5-a8f9-dcf9ca2951c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-sklearn 1.0 | train = 0.9549375661375661 | test = 0.7929904761904762\n",
            "Selected `resampling-strategy` = holdout\n",
            "Selected `resampling-strategy-arguments` = None\n",
            "auto-sklearn results:\n",
            "  Dataset name: credit-g\n",
            "  Metric: roc_auc\n",
            "  Best validation score: 0.810500\n",
            "  Number of target algorithm runs: 28\n",
            "  Number of successful target algorithm runs: 27\n",
            "  Number of crashed target algorithm runs: 0\n",
            "  Number of target algorithms that exceeded the time limit: 1\n",
            "  Number of target algorithms that exceeded the memory limit: 0\n",
            "\n",
            "          rank  ensemble_weight                type      cost   duration\n",
            "model_id                                                                \n",
            "3           12             0.04       liblinear_svc  0.374961   5.282920\n",
            "5            4             0.02   gradient_boosting  0.216294   5.357056\n",
            "7            1             0.14                 sgd  0.189500   8.218706\n",
            "8            9             0.02                 mlp  0.270348  26.110157\n",
            "10           8             0.06       random_forest  0.246505   7.823231\n",
            "11           3             0.32                 qda  0.202625   3.935791\n",
            "12           7             0.02   gradient_boosting  0.235322  20.205365\n",
            "13           2             0.10  passive_aggressive  0.199363   7.582175\n",
            "16          11             0.18                 lda  0.361681   5.170160\n",
            "18           5             0.06         extra_trees  0.218391  10.666765\n",
            "25           6             0.02       liblinear_svc  0.225769   4.828208\n",
            "28          10             0.02          libsvm_svc  0.312209   4.768313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "askl2 = AutoSklearn2Classifier(\n",
        "    **settings\n",
        ")\n",
        "askl2.fit(X_train, y_train, dataset_name=\"credit-g\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yl9CMGG9gIW",
        "outputId": "6a936638-79ff-477d-b79d-96075420b1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autosklearn/experimental/selector.py:26: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for col, series in prediction.iteritems():\n",
            "/usr/local/lib/python3.10/dist-packages/autosklearn/data/target_validator.py:187: UserWarning: Fitting transformer with a pandas series which has the dtype category. Inverse transform may not be able preserve dtype when converting to np.ndarray\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearn2Classifier(metric=roc_auc, n_jobs=4, per_run_time_limit=48, seed=42,\n",
              "                       time_left_for_this_task=120)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jak wyciągać poszczególne elementy?"
      ],
      "metadata": {
        "id": "Z487ioLI65_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (weight, pipeline) in enumerate(askl1.get_models_with_weights()):\n",
        "    for stage_name, component in pipeline.named_steps.items():\n",
        "        if \"feature_preprocessor\" in stage_name:\n",
        "          print(i)\n",
        "          print(component.choice.preprocessor)\n",
        "        if \"classifier\" in stage_name:\n",
        "          print(component.choice)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGHJWR6F0RIW",
        "outputId": "96193af5-22ed-40e0-aacc-3336400895fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "passthrough\n",
            "autosklearn.pipeline Quadratic Discriminant Analysis\n",
            "1\n",
            "KernelPCA(coef0=0.0, gamma=0.011140362342581723, kernel='rbf',\n",
            "          n_components=1598, random_state=42, remove_zero_eig=True)\n",
            "autosklearn.pipeline Linear Discriminant Analysis\n",
            "2\n",
            "FastICA(fun='exp', random_state=42, whiten=False)\n",
            "autosklearn.pipeline Stochastic Gradient Descent Classifier\n",
            "3\n",
            "Nystroem(coef0=1.0, degree=3, gamma=1.0, kernel='cosine', n_components=1358,\n",
            "         random_state=42)\n",
            "autosklearn.pipeline Passive Aggressive Classifier\n",
            "4\n",
            "FeatureAgglomeration(n_clusters=22,\n",
            "                     pooling_func=<function amax at 0x7f196fae79a0>)\n",
            "autosklearn.pipeline Random Forest Classifier\n",
            "5\n",
            "SelectFromModel(estimator=ExtraTreesClassifier(class_weight='balanced',\n",
            "                                               criterion='entropy',\n",
            "                                               max_features=16,\n",
            "                                               min_samples_leaf=16, n_jobs=1,\n",
            "                                               random_state=42),\n",
            "                prefit=True, threshold='mean')\n",
            "autosklearn.pipeline Extra Trees Classifier\n",
            "6\n",
            "RBFSampler(gamma=0.4419041519356276, n_components=602, random_state=42)\n",
            "autosklearn.pipeline Liblinear Support Vector Classification\n",
            "7\n",
            "SelectPercentile(percentile=92)\n",
            "autosklearn.pipeline Gradient Boosting Classifier\n",
            "8\n",
            "FeatureAgglomeration(linkage='complete', n_clusters=61,\n",
            "                     pooling_func=<function amax at 0x7f196fae79a0>)\n",
            "autosklearn.pipeline Multilayer Percepton\n",
            "9\n",
            "PolynomialFeatures()\n",
            "autosklearn.pipeline Gradient Boosting Classifier\n",
            "10\n",
            "SelectPercentile(percentile=71)\n",
            "autosklearn.pipeline Liblinear Support Vector Classification\n",
            "11\n",
            "PolynomialFeatures(degree=3, include_bias=False, interaction_only=True)\n",
            "autosklearn.pipeline LibSVM Support Vector Classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 2\n",
        "\n",
        "Bazując na wnioskach z poprzedniego zadania, sprawdź czy zastosowanie metod preprocessingu wybieranych w autosklearnie poprawi jakość modeli zbudowanych w Autogluonie.\n",
        "\n",
        "- W wersji najprostszej można zrobić preprocessing na całych danych a następnie wykorzystać Autogluon.\n",
        "\n",
        "- W wersji średnio trudnej można wykorzystać  moduł `sklearn.pipeline`\n",
        "- W wersji pro można wykorzystać przykład i zdefiniować odpowiednią klasę w Autogluonie\n",
        "\n",
        "https://auto.gluon.ai/stable/tutorials/tabular/tabular-feature-engineering.html\n",
        "  https://github.com/autogluon/autogluon/blob/master/examples/tabular/example_custom_feature_generator.py"
      ],
      "metadata": {
        "id": "Rd55jR_e_iGH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcnIFS2QBqg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}